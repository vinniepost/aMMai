{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/python development/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/workspaces/python development/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:06<00:00, 16.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the pre-trained ResNet model\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet.eval()\n",
    "\n",
    "# Function to extract features from an image\n",
    "def extract_features(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img_t = transform(img)\n",
    "    img_t = img_t.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = resnet(img_t)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def generate_caption(features):\n",
    "    input_ids = tokenizer.encode(\"Image features: \" + str(features), return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "    caption = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Actual and generated captions\u001b[39;00m\n\u001b[1;32m      4\u001b[0m reference \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA dog is running in the park\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m hypothesis \u001b[38;5;241m=\u001b[39m generate_caption(\u001b[43mfeatures\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate BLEU score\u001b[39;00m\n\u001b[1;32m      8\u001b[0m score \u001b[38;5;241m=\u001b[39m sentence_bleu([reference\u001b[38;5;241m.\u001b[39msplit()], hypothesis\u001b[38;5;241m.\u001b[39msplit())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Actual and generated captions\n",
    "reference = \"A dog is running in the park\"\n",
    "hypothesis = generate_caption(features)\n",
    "\n",
    "# Calculate BLEU score\n",
    "score = sentence_bleu([reference.split()], hypothesis.split())\n",
    "print(f\"BLEU Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: Image features: [-0.13652336597442627, -0.09201764315366745, 0.5357312560081482, 0.39029809832572937, 0.5979483723640442, 0.652328372001648, 0.1887870877981186, 0.13419455289840698, 0.26435065269470215, -0.0016855307621881366]\n",
      "\n",
      "The following table shows the average number of times the number of times the number of times the number of times the number of times the number of times the number of times the number of times the number of times the number of times the number of\n",
      "BLEU Score: 0.003040349233039763\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Load the pre-trained ResNet model\n",
    "resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "resnet.eval()\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Function to extract features from an image\n",
    "def extract_features(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img_t = transform(img)\n",
    "    img_t = img_t.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = resnet(img_t)\n",
    "    return features\n",
    "\n",
    "# Function to generate a caption based on image features\n",
    "def generate_caption(features):\n",
    "    input_ids = tokenizer.encode(\"Image features: \" + str(features[0][:10].tolist()), return_tensors='pt')\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    caption = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Download an example image\n",
    "    image_url = \"https://images.unsplash.com/photo-1517423440428-a5a00ad493e8\"\n",
    "    image_path = \"./example.jpg\"\n",
    "    img_data = requests.get(image_url).content\n",
    "    with open(image_path, 'wb') as handler:\n",
    "        handler.write(img_data)\n",
    "\n",
    "    # Extract features from the image\n",
    "    features = extract_features(image_path)\n",
    "\n",
    "    # Generate a caption for the image\n",
    "    generated_caption = generate_caption(features)\n",
    "    print(f\"Generated Caption: {generated_caption}\")\n",
    "\n",
    "    # Example reference caption for evaluation\n",
    "    reference_caption = \"A dog is sitting on the grass in a park.\"\n",
    "\n",
    "    # Evaluate the generated caption using BLEU score with smoothing\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    bleu_score = sentence_bleu([reference_caption.split()], generated_caption.split(), smoothing_function=smoothing_function)\n",
    "    print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
