{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7471c27f",
   "metadata": {},
   "source": [
    "\n",
    "# Multimodal AI for Image Captioning\n",
    "\n",
    "This notebook demonstrates the development of a multimodal AI system that generates text descriptions of images by combining a pre-trained CNN image model and a text generation model.\n",
    "The image model extracts visual features, which are then used by a language model to produce descriptive text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8741807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: a black and white ll with glasses on it ' s head\n"
     ]
    }
   ],
   "source": [
    "# # Import libraries\n",
    "# import torch\n",
    "# from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "# from PIL import Image\n",
    "# import requests\n",
    "\n",
    "# # Load the BLIP model and processor\n",
    "# processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "# model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# # Define a function to generate captions\n",
    "# def generate_caption_blip(image_url):\n",
    "#     # Load and preprocess the image\n",
    "#     image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n",
    "#     inputs = processor(image, return_tensors=\"pt\")\n",
    "    \n",
    "#     # Generate caption\n",
    "#     with torch.no_grad():\n",
    "#         caption_ids = model.generate(**inputs)\n",
    "#         caption = processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "#     return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "350a32ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: a plane taking off from an airport runway\n"
     ]
    }
   ],
   "source": [
    "# # Example usage\n",
    "# image_url = \"https://lp-cms-production.imgix.net/2024-05/GettyImages-1303030943.jpg?w=1440&h=810&fit=crop&auto=format&q=75\"  # Replace with actual image URL\n",
    "# caption = generate_caption_blip(image_url)\n",
    "# print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5086e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load the BLIP model for image feature extraction\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load GPT-2 for text generation\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_model.eval()\n",
    "\n",
    "# Step 1: Generate a descriptive feature summary from BLIP\n",
    "def extract_image_features(image_url):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate initial description\n",
    "    with torch.no_grad():\n",
    "        feature_ids = blip_model.generate(**inputs)\n",
    "        feature_summary = processor.decode(feature_ids[0], skip_special_tokens=True)\n",
    "    return feature_summary\n",
    "\n",
    "# Step 2: Use GPT-2 to expand on the BLIP description\n",
    "# Final version of generate_multimodal_caption function\n",
    "def generate_multimodal_caption(image_url, max_length=100):\n",
    "    # Step 1: Get the initial description from BLIP\n",
    "    feature_summary = extract_image_features(image_url)\n",
    "    prompt = f\"<|startoftext|>Description: {feature_summary}. Expand with more details:\"\n",
    "\n",
    "    # Encode prompt for GPT-2\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    # Set eos_token_id as pad_token_id\n",
    "    gpt2_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Step 2: Generate a caption using sampling with repetition penalty\n",
    "    with torch.no_grad():\n",
    "        output = gpt2_model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,         # Slightly lower temperature for balance\n",
    "            top_k=40,                # Adjusted sampling\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,  # Penalty to discourage repeated phrases\n",
    "            early_stopping=True\n",
    "        )\n",
    "        caption = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea3848ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: <|startoftext|>Description: a plane taking off from an airport runway. Expand with more details: Aircraft takeoffs and landings are not always connected to one another in the same direction, or even just over different airports at each other's ports (usually near ones that have some sort of border).\n",
      "A few examples would be as follows: [1] \"Eagles\" fly into Washington DC on July 20th for their flight back home by 3 p . The United States\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_url = \"https://lp-cms-production.imgix.net/2024-05/GettyImages-1303030943.jpg?w=1440&h=810&fit=crop&auto=format&q=75\"  # Replace with actual image URL\n",
    "caption = generate_multimodal_caption(image_url)\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fcb468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
